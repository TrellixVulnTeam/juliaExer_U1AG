# 重回帰分析

本記事では `SymPy.jl, Zygote.jl` の使い道として重回帰問題を解くことを考える．前半では問題設定と数学的な解法を与え，
後半では `SymPy.jl` を中心とした実装例を与える．なお，この記事は `Weave.jl` を用いて Markdown 形式のドキュメントを HTML に変換したものである．


# 問題設定

前半では重回帰モデルの導入とデータにフィットするようにモデルのパラメータをフィッテングするための目的関数を導入する．

## モデルの設定をする.
データの次元が $D$ である説明変数と目的変数をペアとする二次元の $N$ 点集合
$\mathcal{D}=\{\,(\mathbf{x}_n,y_n) \mid x_n \in \mathbb{R}^D, y_n \in \mathbb{R}, i=1,\dots, N\,\}$ が与えられたとき，
この点集合を特徴付けるモデルを考えたい．モデル $f$ として次の形を考える:

$$
\begin{align*}
f &= f(\mathbf{x}) \\
  &= f(\mathbf{x}; \mathbf{w}) \\
  &:= w_0+ w_1 x_1 + w_2 x_2 + \dots + w_D x_D .
\end{align*}
$$

ここで,

$$
\begin{align*}
\mathbf{x}=
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_D
\end{bmatrix} \in \mathbb{R}^D,
\quad
\mathbf{w}=
\begin{bmatrix}
w_0 \\
w_1 \\
w_2 \\
\vdots \\
w_D
\end{bmatrix} \in \mathbb{R}^{D+1}
\end{align*}
$$

であり, $\mathbf{w}$ がこのモデルに関するパラメータになる. $D=1$ の場合は単回帰で設定したモデル $f(x;a,b) = ax+b$ そのものになる.
従って, 我々がこれから考えようとするモデルは単回帰の一般化を試みることを意味しており，
このモデルを重回帰と呼ぶことにする．

## 説明変数の拡張

次元が $D$ の説明変数 $\mathbf{x} = \left[ x_1, x_2, \dots, x_D\right]^\top$ を
最初の成分が 1 を持つ $D+1$ 次元のデータ $\left[x_0, x_1, x_2, \dots, x_D\right]^\top $, $x_0=1$ と同一視する.
ここで ${}^\top$ はベクトルの転置を表している.
この意味で，拡張した説明変数を改めて $\mathbf{x}$ とおきなおす. すなわち:

$$
\begin{align*}
\mathbf{x}=
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_D
\end{bmatrix}
=
\begin{bmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_D
\end{bmatrix}
\end{align*}
\in \mathbb{R}^{D+1}.
$$

以上の同一視のもとでモデル $f$ は次のように表記できる:

$$
\begin{align*}
f(\mathbf{x};\mathbf{w})
&=w_0+ w_1 x_1 + w_2 x_2 + \dots + w_D x_D \\
&=w_0x_0 + w_1 x_1 + w_2 x_2 + \dots + w_D x_D \quad (\because x_0=1)\\
&=\left[x_0, x_1, x_1, \dots, x_D\right]
\begin{bmatrix}
w_0 \\
w_1 \\
\vdots \\
w_D
\end{bmatrix}
\\
&=\mathbf{x}^\top \mathbf{w} = \langle \mathbf{x},\mathbf{w}\rangle .
\end{align*}
$$

ここで $\langle \bullet, \bullet \rangle$ はユークリッド空間の内積を表す.

## 目的関数の導入

目的関数 $E$ として次のように定義する:

$$
\begin{equation*}
E := E(\mathbf{w}) = \sum_{n=1}^N (y_n - \hat{y}_n)^2 .
\end{equation*}
$$

ただし, $\hat{y}_n$ はモデル $f$ の目的変数 $\mathbf{x}_n$ に対する出力を表す:

$$
\begin{equation*}
\hat{y}_n = \hat{y}_n(\mathbf{w}) = f(\mathbf{x}_n; \mathbf{w}) = \langle \mathbf{x}_n, \mathbf{w}\rangle \quad \mathrm{for}\quad n = 1,\dots,N.
\end{equation*}
$$

我々目標は $E(\mathbf{w})$ を最小にする $\mathbf{w}=\hat{\mathbf{w}}$ を求めることである:

$$
\hat{\mathbf{w}} = \underset{\mathbf{w}}{\operatorname{argmin}} E(\mathbf{w})
$$

## 目的関数の変形

目的関数の微分を系統的にするために式変形を行う. まず

$$
\begin{align*}
\mathbf{y}
:=
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
\hat{y}_N
\end{bmatrix}
\in \mathbb{R}^N
,\quad
\hat{\mathbf{y}}
:=
\begin{bmatrix}
\hat{y}_1 \\
\hat{y}_2 \\
\vdots \\
\hat{y}_N
\end{bmatrix}
\end{align*}
\in \mathbb{R}^N
$$

と定義する. $N$ 行 $D+1$ 列の行列 $X \in \mathrm{Mat}_{N,D+1}(\mathbb{R})$ として説明変数 $\mathbf{x_n}$ を横ベクトルに転置した $\mathbf{x}_n^\top=\left[x_{n1},\dots ,x_{nD}\right]$ を $n = 1,\dots N$ の順に縦に並べて定義される行列として定める:

$$
X :=
\begin{bmatrix}
\mathbf{x}_1^\top \\
\mathbf{x}_2^\top \\
\vdots \\
\mathbf{x}_N^\top
\end{bmatrix}
=
\begin{bmatrix}
1      & x_{11} & x_{12} & \dots   & x_{1D} \\
1      & x_{21} & x_{22} & \dots   & x_{2D} \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
1      & x_{N1} & x_{12} & \dots   & x_{ND}
\end{bmatrix}
.
$$

この $X$ を計画行列と呼ぶ. 計画行列を使うことで $\hat{y}$ は次のように書ける:

$$
\begin{align*}
\hat{\mathbf{y}}
=
\begin{bmatrix}
\hat{y}_1 \\
\hat{y}_2 \\
\vdots \\
\hat{y}_N
\end{bmatrix}
=
\begin{bmatrix}
\langle \mathbf{x}_1, \mathbf{w}\rangle \\
\langle \mathbf{x}_2, \mathbf{w}\rangle \\
\vdots \\
\langle \mathbf{x}_N, \mathbf{w}\rangle
\end{bmatrix}
\end{align*}
=
\begin{bmatrix}
1      & x_{11} & x_{12} & \dots   & x_{1D} \\
1      & x_{21} & x_{22} & \dots   & x_{2D} \\
\vdots & \vdots & \vdots & \ddots  & \vdots \\
1      & x_{N1} & x_{12} & \dots   & x_{ND}
\end{bmatrix}
\begin{bmatrix}
w_0 \\
w_1 \\
w_2 \\
\vdots \\
w_D
\end{bmatrix}
= X\mathbf{w}.
$$

以上の準備のもとで目的関数 $E$ は計画行列と内積を用いて次のように書き下すことができる:

$$
\begin{align*}
E(\mathbf{w})
&= \sum_{n=1}^N (y_n - \hat{y}_n)^2 \\
&= \langle\mathbf{y} - \hat{\mathbf{y}},\mathbf{y} - \hat{\mathbf{y}} \rangle \\
&= \langle\mathbf{y},\mathbf{y} \rangle - 2 \langle\mathbf{y}, \hat{\mathbf{y}} \rangle + \langle \hat{\mathbf{y}}, \hat{\mathbf{y}} \rangle \\
&= \langle\mathbf{y},\mathbf{y} \rangle - 2 \langle\mathbf{y}, X\mathbf{w} \rangle + \langle X\mathbf{w}, X\mathbf{w} \rangle \\
\end{align*}
$$

# 微分の計算

目的関数 $E=E(\mathbf{w})$ の極値問題を解くことに帰着される.
微分の計算を系統的にするため多変数関数及び写像の微分を導入する.
$\mathbb{R}^N$ から $\mathbb{R}^M$ への写像
$\varphi : \mathbb{R}^N \rightarrow \mathbb{R}^M ; \mathbf{w}=(w_1,\dots,w_N) \mapsto \varphi(w)=(\varphi_1(\mathbf{w}),\dots,\varphi_M(\mathbf{w}))$ に対して定まる $M$ 行 $N$ 列の行列を $\varphi$ のヤコビ行列と呼ぶ:

$$
\frac{\partial \varphi}{\partial \mathbf{w}}(\mathbf{w}) := \left[\frac{\partial \varphi_i}{\partial w_j}(\mathbf{w})\right]_{i,j=1}^{M,N}
$$

なお $M=1$ の場合はヤコビ行列は $N$ 列の横ベクトルとなる:

$$
\begin{equation*}
\frac{\partial \varphi}{\partial \mathbf{w}}(\mathbf{w})
= \left(
\frac{\partial \varphi}{\partial w_1}(\mathbf{w})
,\dots,
\frac{\partial \varphi}{\partial w_N}(\mathbf{w})
\right)
\end{equation*}
$$

これは勾配ベクトル

$$
\begin{equation*}
\nabla_{\mathbf{w}} \varphi(\mathbf{w}) =
\begin{bmatrix}
\frac{\partial \varphi}{\partial w_1}(\mathbf{w}) \\
\vdots \\
\frac{\partial \varphi}{\partial w_N}(\mathbf{w})
\end{bmatrix}
\end{equation*}
$$

の転置となることに注意する.

## 内積の微分の導出

補題: $\mathbf{a}=\left[a_1,\dots,a_N\right]^\top, \mathbf{w}=\left[w_1,\dots,w_N\right] \in \mathbb{R}^N$, $A\in \mathrm{Mat}_{M,N}(\mathbb{R})$ に対して次が成り立つ:

$$
\begin{align*}
& \frac{\partial }{\partial \mathbf{w}} \langle \mathbf{a}, \mathbf{w}\rangle = \mathbf{a}^\top = \left[a_1,\dots,a_N\right] , \\
& \frac{\partial }{\partial \mathbf{w}} \langle \mathbf{a}, A \mathbf{w}\rangle = \mathbf{a}^\top A .
\end{align*}
$$

証明: 最初の式は内積の定義より自明. また $\langle \mathbf{a},A\mathbf{w} \rangle = $\langle A^\top \mathbf{a},\mathbf{w} \rangle = (A^\Top \mathbf{a})^\top = \mathbf{a}^\top A$ により二番目の式が成立する.
補題: $\varphi$, $\psi : \mathbb{R}^N \rightarrow \mathbb{R}^M$ に対して次が成り立つ:

$$
\begin{equation*}
\frac{\partial }{\partial \mathbf{w}} \langle \varphi(\mathbf{w}), \psi(\mathbf{w}) \rangle
=
\psi(\mathbf{w})^\top \frac{\partial\varphi(\mathbf{w})}{\partial \mathbf{w}}
+
\varphi(\mathbf{w})^\top \frac{\partial\psi(\mathbf{w})}{\partial \mathbf{w}} .
\end{equation*}
$$

ただし
$\varphi(\mathbf{w})=\left[ \varphi_1(\mathbf{w}),\dots,\varphi_N(\mathbf{w})\right]^\top$,
$\psi(\mathbf{w})=\left[ \psi_1(\mathbf{w}),\dots,\psi_N(\mathbf{w})\right]^\top$
である.

証明: 根性でする.

## $\nabla_{\mathbf{w}} E$ の導出

上記補題を用いると

$$
\begin{align*}
\nabla_{w}\langle \mathbf{y}, X\hat{\mathbf{y}} \rangle = X^\top\mathbf{y}, \\
\nabla_{w}\langle X\mathbf{w},X\mathbf{w} \rangle = 2X^\top Xw
\end{align*}
$$

などから

$$
\begin{align*}
\nabla_{\mathbf{w}} E(\mathbf{w}) = \mathbf{0} \Leftrightarrow \mathbf{w}=(X^\top X)^{-1}X^\top y
\end{align*}
$$

がわかる.

# 実装をする.

関数の微分は `SymPy.diff` でも良いが, 数値計算における自動微分をサポートする `Zygote.jl` を使うことにする.
`Zygote.jl` は計算グラフを構築し数値的に微分を数値的に計算する目的で作られているが Julia の multi-dispatch のおかげか `SymPy.jl` の要素を受け取ることができる.

```julia
using SymPy: @vars, Sym, solve
using SymPy: simplify
using Zygote

dot(x,y) = sum(x .* y)

function generate_vector(v::String, n::Int; start::Int = 1, kwargs::String = "")
    data::Vector{Sym} = []
    for i ∈ start:n
        seqdigit = [c for c in string(i)]
        v_s = "$v" * join(seqdigit)
        eval(Meta.parse("@vars $v_s $kwargs"))
        push!(data, eval(Meta.parse("$(v_s)")))
    end
    return data
end

D = 2 # Dimension of data
N = 3 # Num of data


function generate_matrix(
    v::String,
    row::Int,
    col::Int;
    rstart::Int = 1,
    cstart::Int = 1,
    kwargs::String = "",
)::Matrix{Sym}
    X_ = Vector{Sym}[]
    for n = rstart:row
        xn = "$v$n"
        eval(Meta.parse("xn=generate_vector(\"x$n\", $col, start = $cstart, kwargs=\"$kwargs\")"))
        push!(X_, eval(Meta.parse("xn")))
    end
    Xᵀ = Matrix{Sym}(hcat(X_...))
    X = transpose(Xᵀ)
    X
end

X = generate_matrix("x", N, D, cstart = 0, kwargs = "real=true")
y = generate_vector("y", N, kwargs = "real=true")
w = generate_vector("w", D, start = 0, kwargs = "real=true")

# Numerical
# X = rand(N, D + 1)
# y = rand(N)

X[:, 1] .= 1.0 # set xn0 = 1 for n in 1:N

ŷ(X) = X * w
E(X) = 1 / 2 * (dot(y, y) - 2 * dot(y, ŷ(X)) + dot(ŷ(X), ŷ(X)))

@info "calc-gradient"

∇E = gradient(Params([w])) do
    E(X)
end

@info "solve"
solution = solve(∇E[w], w)
solution = [solution[k] for k in w]
@info "solve-with-norma-equation"
# solve ŵ with normal equation
theoretical = inv(Matrix{eltype(X)}(X' * X)) * Matrix{eltype(X)}(X') * y


@info "check isequal"
if eltype(theoretical) == Sym
    @assert all(solution .- theoretical .|> simplify .== 0)
else
    solution=Array{Float64}(solution)
    @assert all(isapprox.(solution ,theoretical, atol=1e-6))
end
@info "done"

```
